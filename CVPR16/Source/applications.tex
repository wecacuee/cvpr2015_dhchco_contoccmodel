\section{Unified Occlusion Models}
%\section{Occlusion-Aware Association and Localization}
\label{sec:unified}

In this section, we highlight the versatility of our occlusion modeling by demonstrating its unified application to two different problems: associating point tracks with objects and 3D object localization using objects and point tracks. Table~\ref{tab:notations} summarizes inputs and outputs for these problems.

\newcommand\RotText[1]{\rotatebox{90}{\parbox{1.7cm}{\centering#1}}}
\begin{wraptable}[16]{r}{0.5\textwidth}
\centering\footnotesize
\begin{tabular}{|l|l|l|}
\hline
 & Symbol & Description \\
\hline
\hline
\multirow{2}{*}{\rotatebox{90}{Input}} & $\bu_j(t)$ & 2D feature track $j$ at time $t$ \\
 & $\bb{i}$ & 2D detection bounding box of object $O_i$ at time $t$ \\
\hline
\hline
\multirow{5}{*}{\RotText{Initialization with~\cite{Song_Chandraker_2014}}} & $\bp^c(t)$ & Position of camera at time $t$ \\
 & $\bomega^c(t)$ & Orientation of camera at time $t$ \\
 & $\bp_0^i(t)$ & Initial position of object $O_i$ at time $t$ \\
 & $\bomega_0^i(t)$ & Initial orientation of object $O_i$ at time $t$ \\
 & $\bB_0^i$ & Initial 3D dimensions of object $O_i$ \\
\hline
\hline
\multirow{4}{*}{\rotatebox{90}{Output}} & $P^{ij}_{\text{assoc}}$ & Probability of assigning feature track $j$ to object $O_i$ \\	
 & $\bp^i(t)$ & Position of object $O_i$ at time $t$ \\
 & $\bomega^i(t)$ & Orientation of object $O_i$ at time $t$ \\
 & $\bB^i$ & 3D dimensions of object $O_i$ \\
\hline
\end{tabular}
\vspace{-0.25cm}
\caption{Notation of inputs and outputs for object-point association and 3D object localization. Note that object dimensions are independent of time.}
\label{tab:notations}
\vspace{-0.5cm}
\end{wraptable}

\subsection{Object-Point Association}
\label{sec:association}

%Given a set of corresponding 2D points $\trackpj{t-1}$ and $\trackpj{t}$ in consecutive frames, object poses $\relp{i}{t}$ and object dimensions $\dimsn{i}$, we wish to associate points with various objects. We call this the association problem. Even if the pose of the object is not immediately available, we propose the use of hypothesized pose of the object as we show in the localization estimation experiments. 

Given 2D image points $\{\bu_j\}$ that are tracked between consecutive frames and a set of objects $\{O_i\}$ appearing in the frames, we aim to associate $\bu_j$ with $O_i$. Based on our continuous occlusion model in Section~\ref{sec:setup}, the association probability $\assocP$ between point track $\bu_j$ and object $O_i$ at depth $\lambda$ can be defined as
\begin{align}
\assocP = \Prefl\Ptrans,
\label{eq:defineassocP}
\end{align}
where $\Prefl$ and $\Ptrans$ are from \eqref{eq:evalPrefl} and \eqref{eq:evalCumulativePtrans} respectively. Note that the fraction $\assocP$, although called association probability, does not capture the entire information that we have available for computing the association of point tracks to objects. 

Rather, to compute the association probability between point track $\bu_j$ and object $O_i$, we should also use the reprojection error. When the association of point track $\bu_j$ and object $O_i$ is correct and the point of reflection is at depth $\lambda$, the corresponding reprojection error $\Ereproj(\lambda)$ must be zero, otherwise the error becomes a measure of distance from the true solution. The error $\Ereproj(\lambda)$ can be used for associating point tracks and objects by converting it to the probability domain as
\begin{align}
  P^{ij}_{\text{reproj}}(\lambda) = \frac{1}{Z'}\exp(-\Ereproj(\lambda)),
\label{eq:Passocbyreproj}
\end{align}
where $Z'$ is the normalization coefficient.

Using both of the evidence terms in \eqref{eq:defineassocP} and \eqref{eq:Passocbyreproj}, we can define the new association probability $P^{ij}_{\text{assoc}}$, as follows:
\begin{align}
  P^{ij}_{\text{assoc}} = \frac{1}{Z''}\int_0^{\infty} \assocP \exp(-\Ereproj(\lambda))d\lambda,
  \label{eq:prob-assoc}
\end{align}
where $Z''$ is the new normalization coefficient.

Once we have computed the association probability $P^{ij}_{\text{assoc}}$ for every pair of point tracks and objects, we can assign each point track to the object with the highest association probability. The point tracks having very small association probabilities are assigned to the background.

In contrast to the principled approach above, a heuristic baseline may simply assign a point track to the detection bounding box enclosing it (and the background if outside all bounding boxes). For regions where bounding boxes overlap, it may assign point tracks to the object that has the smallest mean depth among the competing bounding boxes. As we demonstrate in our experiments, such heuristics are sub-optimal compared to using \eqref{eq:prob-assoc} from our occlusion model.

%For the baseline method, the associations between point tracks and TPs are achieved by using detection bounding boxes, where point tracks within each bounding box are simply assigned to it. For the regions where the bounding boxes overlap, we assign the point tracks to the TP that has smaller mean depth than the competing bounding boxes.



\subsection{3D Object Localization}
\label{sec:localization}

%To show the effectiveness of our method we apply it to the localization problem. We estimate the position, orientation and dimensions of the car with our framework and compute the error in birds eye view domain along the ground plane. To estimate the localization of traffic participants (TPs) in a road scene we use the graphical model approach. We build the graphical model as shown in Fig~\ref{fig:graphmodel} that allows us to factorize the intractable probability distribution into following form

%To show the effectiveness of our method we also apply it to the localization problem, where we must jointly solve for association and object poses, while considering occlusions. In road scenes, we estimate the position, orientation and dimensions traffic participants (cars) with our framework and compute the error in birds eye view along the ground plane. We propose a graphical model as shown in Figure \ref{fig:graphmodel} that allows a factorization of the intractable probability distribution into following form

\begin{wrapfigure}[22]{r}{0.5\textwidth}
  \centering
  \begin{tabular}{c}
    \newcommand{\imagewidth}{7.5cm}
      \hspace{-0.8cm}
    \input{Source/scenelayoutoverlayCity0961} \\
      \hspace{-0.8cm}
    \input{Source/graphical_model_multiple}
  \end{tabular}
  \caption{\small (Top) A sample road scene with occlusions, where the unknowns of each object are modeled as random variables. (Bottom) The graphical model corresponding to the above frame. In particular, the numbered nodes denote the unknown state variables of each object (position, orientation, and dimensions), the shaded nodes are observed variables (detection bounding boxes and point tracks), and the colored squares represent various energies that capture object-object interactions.}
  \label{fig:graphmodel}
\end{wrapfigure}


In this section, we exploit our continuous occlusion model for another application, namely, 3D object localization in road scenes, which further demonstrates its versatility. Given a set of 2D tracked feature points $\{\bu_j(t)\}$ and 2D detection bounding boxes $\{\bb{i}\}$ at frame $t$, the goal is to localize 3D traffic participants. In particular, for each traffic participant, we wish to estimate its position $\bp^i(t)$ and orientation $\bomega^i(t)$ on the ground plane and its 3D dimensions $\bB^i(t)$. Please refer to Table~\ref{tab:notations} for a summary of inputs and outputs. 

We construct a graphical model for representing relationships among objects, as well as between objects and point tracks. Figure~\ref{fig:graphmodel} illustrates an example of the graph and energies. The negative log likelihood is decomposed as follows:
%
\begin{multline*}
  -\log{P(\{ \bp^i(t) , \bomega^i(t) , \bB^i(t) \} | \{\bu_j(t)\} , \{\bb{i}\} )} = \\
  -\tilde{Z} 
  + \sum_{t=s_i}^{e_i} \lambda_{\text{track}}\EnergyTrack
  + \\ 
  \sum_{t=s_i}^{e_i} \sum_{i=1}^N  
  \left(
    \lambda_{\text{detect}}\EnergyBBox
    + \lambda_{\text{dyn}}\EnergyDyn    
    + \lambda_{\text{size}}\EnergySize
  \right)
  \enspace,
\end{multline*}
%
where $\pEnergy{track}$ and $\Energy{detect}$ reason about image observations such as point tracks and bounding boxes, while $\Energy{dyn}$  and $\Energy{size}$ impose smoothness constraints and size priors respectively. Here, $\lambda_{\text{track}}$, $\lambda_{\text{detect}}$, $\lambda_{\text{dyn}}$, $\lambda_{\text{size}}$ are energy weights, $N$ is the number of objects in the sequence, $s_i$ and $t_i$ are respectively the starting and ending frames of object $O_i$, and $\tilde{Z}$ is the normalization coefficient. Next, we present our unified continuous occlusion modeling for both point track and bounding box energies. Due to space constraints, we present the details of other energies in the supplementary material.

\vspace{-0.3cm}
\paragraph{Continuous point track energy with occlusion}
%\label{sec:totalContPtTracksEnergy}
Let $\relp{i}{t}$ be the pose of object $O_i$ at time $t$ in world coordinates, which is computed using the camera pose at time $t$ and the relative pose of object $O_i$ with respect to the camera at time $t$. We denote $\projectionOf{.}$ and $\invProjectionOftm{.}$ as the forward and inverse projection functions that project a 3D point to the 2D image and vice versa. Then, the reprojection error for 2D point $\trackp{t}$ with hypothesized depth $\lambda$, is given by
\begin{equation}
\Ereproj(\lambda) = \left\|\trackpj{t} - \projectionOf{\invProjectionOftm{\trackpj{t-1}, \lambda}}\right\|^2.
\label{eq:reprojerror}
\end{equation}
Note that the inverse projection $\invProjectionOf{.}$ depends on both the 2D point $\trackp{t}$ and the unknown depth $\lambda$. Also note that the inverse projection relies on the object pose at time $t-1$ while the forward projection relies on the object pose at time $t$, which can be different.

For an object $O_i$, let $\{ \Omega (t) \}_i$ be the poses of all occluding objects at time $t$ (inclusive of object $O_i$) and $ \{ \bB \}_i$ be their corresponding 3D dimensions. Then, we model the continuous point track energy with explicit occlusion reasoning as the expected reprojection error over the association probability
\begin{align}
  %\!\!\!\! \Energy{track}(\relp{i}{t}, \relp{i}{t-1}, \dimsn{i}, \{ \Omega (t) \}_i, \{ \Omega (t-1) \}_i, \{ \bB \}_i ) 
  %\Energy{track}(\{ \relp{i}{t} \}_i, \{ \relp{i}{t-1} \}_i, \{\dimsn{i}\}_i ) = 
  \pEnergy{track}(\{ \Omega (t) \}_i, \{ \Omega (t-1) \}_i, \{ \bB \}_i )
  %\\
    = \sum_{i=1}^{N} 
    %\sum_{t = s_i}^{e_i}
    \sum_{j = 1}^{M}
    \int_0^\infty \assocP\Ereproj(\lambda) d\lambda,
\end{align}
where $N$ and $M$ are, respectively, the number of objects and points and $\assocP$ is the association probability of point $\trackp{t}$ with object $O_i$ at depth $\lambda$, given by \eqref{eq:defineassocP}.

%\begin{align}
%  \assocP &= \Prefl\Ptrans\\
%  \Ereproj(\lambda) &= \left\|\trackpj{t} - \projectionOf{\invProjectionOftm{\trackpj{t-1}, \lambda}}\right\|^2 .
%  \label{eq:reprojerror}
%\end{align}

%The $\projectionOf{.}$ and $\invProjectionOftm{.}$ denote the projection and inverse projection functions that project 3D point to camera image and vice versa. Note that inverse projection $\invProjectionOf{.}$ depend on both the point $\trackp{t}$ and the unknown depth $\lambda$. Also note that the inverse projection is dependent on TP pose at time $t-1$ while the projection depends on pose at time $t$ which can be different.

\input{Source/contBBoxModel}
%\input{Source/squareBBoxModel.tex}


%\subsubsection{Other energies}
%Other energies we use are described in detail in the supplementary material. We briefly describe them here:
%\begin{description}
  %\item[Lane energy ($\Energy{lane}$)] This energy term constrains the orientation of traffic participants to be parallel to the nearest detected lane. The lanes are either detected visually or obtained from GPS and Map information.
  %\item[Transition probability ($\Energy{dyn}$)] We use these energies to constrain the motion of cars to be smooth in linear motion and rotation motion. We also constrain cars to move in the direction of heading.
  % \item[Size prior ($\EnergySize$)] We use size prior of cars by using the mean of cars over the KITTI dataset.
%\end{description}

\vspace{-0.3cm}
\paragraph{Inference on graphical model}
We apply the Metropolis-Hastings method~\cite{mackay1998introduction} to perform inference on the graphical model. Since we optimize over continuous variables, we use the Gaussian distribution as the proposal function. We choose this over alternatives such as block-coordinate descent since they are slower in our experiments.

%in our experiments. Please refer to the supplementary material for our comparisons of inference methods.

%% \begin{figure}
%%   \centering
%%   \newcommand{\imagewidth}{\columnwidth}
%%   \input{Source/scenelayoutoverlayCity0961}
%%   \caption{A sample road scene with the unknowns of each car modeled as random variables. 
%%   The relating energies are shown in Figure~\ref{fig:graphmodel}}
%% \end{figure}
%% \begin{figure}
%%     \input{Source/graphical_model_multiple}
%%     \caption{Graphical model for a single frame with state of car represented
%%     as single node.  The six numbered nodes represent the unknown state variables of each car. The shaded nodes in the graphical model are observed variables. }
%%   \label{fig:graphmodel}
%% \end{figure}
