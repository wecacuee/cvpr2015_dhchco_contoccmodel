\section{Unified Occlusion Models}
%\section{Occlusion-Aware Association and Localization}
\label{sec:unified}

In this section, we highlight the versatility of our occlusion modeling by demonstrating its unified application to two different problems: associating point tracks with objects and 3D object localization using objects and point tracks.

\subsection{Object-Point Association}
\label{sec:association}

%Given a set of corresponding 2D points $\trackpj{t-1}$ and $\trackpj{t}$ in consecutive frames, object poses $\relp{i}{t}$ and object dimensions $\dimsn{i}$, we wish to associate points with various objects. We call this the association problem. Even if the pose of the object is not immediately available, we propose the use of hypothesized pose of the object as we show in the localization estimation experiments. 

Given 2D image points $\{\bu_j\}$ which are tracked between consecutive frames and a set of objects $\{O_i\}$ appearing in the frames, we aim to associate $\bu_j$ with $O_i$. Based on our continuous occlusion model in Section~\ref{sec:setup}, the association probability $\assocP$ between point track $\bu_j$ and object $O_i$ at depth $\lambda$ can be defined as
\begin{align}
\assocP = \Prefl\Ptrans,
\label{eq:defineassocP}
\end{align}
where $\Prefl$ and $\Ptrans$ are from \eqref{eq:evalPrefl} and \eqref{eq:evalCumulativePtrans} respectively. Note that the fraction $\assocP$ although called association probability does not capture the entire information that we have available for computing association of point tracks to objects. 

To compute the association probability between point track $\bu_j$ and object $O_i$, we should also use the reprojection error. When the association of point track $\bu_j$ and object $O_i$ is correct and the point of reflection is at depth $\lambda$, the corresponding reprojection error $\Ereproj(\lambda)$ must be zero \eqref{eq:reprojerror}, otherwise the error becomes a measure of distance from the true solution. The error $\Ereproj(\lambda)$ can be used for associating point tracks and objects by converting it to the probability domain as
\begin{align}
  P^{ij}_{\text{reproj}}(\lambda) = \frac{1}{Z}\exp(-\Ereproj(\lambda)),
\label{eq:Passocbyreproj}
\end{align}
where $Z$ is the normalization coefficient.

Using both of the evidence terms in \eqref{eq:defineassocP} and \eqref{eq:Passocbyreproj}, we can define the new association probability $P^{ij}_{\text{assoc}}$ as follows
\begin{align}
  P^{ij}_{\text{assoc}} = \frac{1}{Z'}\int_0^{\infty} \assocP \exp(-\Ereproj(\lambda))d\lambda,
  \label{eq:prob-assoc}
\end{align}
where $Z'$ is the new normalization coefficient.

Once we have computed the association probability $P^{ij}_{\text{assoc}}$ for every pair of point tracks and objects, we can assign each point track to the object with the highest association probability. The point tracks having very small association probabilities are assigned to the background.

In contrast to the principled approach above, a heuristic baseline may simply assign a point track to the detection bounding box enclosing it (and background if outside all bounding boxes). For regions where bounding boxes overlap, it may assign point tracks to the object that has the smallest mean depth among the competing bounding boxes. As we will demonstrate in the experiments, such heuristics are sub-optimal compared to using \eqref{eq:prob-assoc} inspired by our occlusion model.

%For the baseline method, the associations between point tracks and TPs are achieved by using detection bounding boxes, where point tracks within each bounding box are simply assigned to it. For the regions where the bounding boxes overlap, we assign the point tracks to the TP that has smaller mean depth than the competing bounding boxes.



\subsection{3D Object Localization}
\label{sec:localization}

%To show the effectiveness of our method we apply it to the localization problem. We estimate the position, orientation and dimensions of the car with our framework and compute the error in birds eye view domain along the ground plane. To estimate the localization of traffic participants (TPs) in a road scene we use the graphical model approach. We build the graphical model as shown in Fig~\ref{fig:graphmodel} that allows us to factorize the intractable probability distribution into following form

%To show the effectiveness of our method we also apply it to the localization problem, where we must jointly solve for association and object poses, while considering occlusions. In road scenes, we estimate the position, orientation and dimensions traffic participants (cars) with our framework and compute the error in birds eye view along the ground plane. We propose a graphical model as shown in Figure \ref{fig:graphmodel} that allows a factorization of the intractable probability distribution into following form

We exploit in this section the application of our continuous occlusion model to 3D localization in road scenes, which further demonstrates the versatility of the proposed model. Given a set of tracked feature points $\{\bu_j(t)\}$ and 2D detection bounding boxes $\{\bb{i}\}$ at frame $t$, the goal is to localize 3D traffic participants (TPs), in particular, for each TP, estimating its position $\bp^i(t)$ and orientation $\bomega^i(t)$ on the ground plane and its 3D dimensions $\bB^i(t)$. We build a graphical model for representing object relationships and decompose the negative log likelihood as follows
%
\begin{multline*}
  -\log{P(\{ \bp^i(t) , \bomega^i(t) , \bB^i(t) \} | \{\bu_j(t)\} , \{\bb{i}\} )} = \\
  -Z 
   + \sum_{t=s_i}^{e_i} \lambda_t\EnergyTrack
  + 
  \sum_{t=s_i}^{e_i}
  \sum_{i=1}^N  
  %\sum_{\substack{i \in [1, \dots, N] \\t \in [s_i, \dots e_i]}}
  \lambda_d\EnergyBBox
  + \lambda_{\text{dyn}}\EnergyDyn
  + \lambda_{\text{size}}\EnergySize
  \enspace,
\end{multline*}
%
where $\pEnergy{track}$ and $\Energy{detect}$ reason about image observations such as point tracks and bounding boxes while $\Energy{dyn}$  and $\Energy{size}$ impose smoothness constraints and size priors respectively. Here, $\lambda_{track}$, $\lambda_{detect}$, $\lambda_{dyn}$, $\lambda_{size}$ are energy weights, $N$ is the number of objects in the sequence, $s_i$ and $t_i$ are respectively the starting and ending frames of object $O_i$, and $Z$ is the normalization coefficient. Figure~\ref{fig:graphmodel} illustrates an example of the graph and energies. Next, we present our unified continuous occlusion modelling for both point track and bounding box energies. Due to space constraints, we present the details of other energies in the supplementary.

\begin{figure}
  \centering
  \begin{tabular}{c}
    \newcommand{\imagewidth}{7.5cm}
      \hspace{-0.8cm}
    \input{Source/scenelayoutoverlayCity0961} \\
      \hspace{-0.8cm}
    \input{Source/graphical_model_multiple}
  \end{tabular}
  \caption{\small (Top) A sample road scene with occlusions, where the unknowns of each car are modeled as random variables. (Bottom) Graphical model for a single frame. The six numbered nodes represent the unknown state variables of each car. The shaded nodes in the graphical model are observed variables (detection bounding boxes and point tracks), while the colored squares represent various energies that capture object-object interactions.}
  \label{fig:graphmodel}
\end{figure}



\vspace{-0.3cm}
\paragraph{Continuous point tracks energy with occlusion}
%\label{sec:totalContPtTracksEnergy}
Let $\relp{i}{t}$ be the pose of object $O_i$ with respect to the camera at time $t$. We denote $\projectionOf{.}$ and $\invProjectionOftm{.}$ for the forward and inverse projection functions that project a 3D point to the camera image and vice versa. Then, the reprojection error for the 2D point $\trackp{t}$ with hypothesized depth $\lambda$, is given by
\begin{equation}
\Ereproj(\lambda) = \left\|\trackpj{t} - \projectionOf{\invProjectionOftm{\trackpj{t-1}, \lambda}}\right\|^2.
\label{eq:reprojerror}
\end{equation}
Note that inverse projection $\invProjectionOf{.}$ depends on both the 2D point $\trackp{t}$ and the unknown depth $\lambda$. Also note that the inverse projection is dependent on object pose at time $t-1$ while the forward projection depends on pose at time $t$, which can be different.

For an object $O_i$, let $\{ \Omega (t) \}_i$ be the poses of all occluding objects at time $t$ (inclusive of object $i$) and $ \{ \bB \}_i$ be their corresponding 3D dimensions. Then, we model the continuous point tracks energy with explicit occlusion reasoning as the expected reprojection error over the association probability:
\begin{multline}
  %\!\!\!\! \Energy{track}(\relp{i}{t}, \relp{i}{t-1}, \dimsn{i}, \{ \Omega (t) \}_i, \{ \Omega (t-1) \}_i, \{ \bB \}_i ) 
  %\Energy{track}(\{ \relp{i}{t} \}_i, \{ \relp{i}{t-1} \}_i, \{\dimsn{i}\}_i ) = 
  \pEnergy{track}(\{ \Omega (t) \}_i, \{ \Omega (t-1) \}_i, \{ \bB \}_i )
  \\
    = \sum_{i=1}^{N} 
    %\sum_{t = s_i}^{e_i}
    \sum_{j = 1}^{M}
    \int_1^\infty \assocP\Ereproj(\lambda) d\lambda
\end{multline}
where $N$ and $M$ are respectively the number of objects and points and $\assocP$ is the association probability of point $\trackp{t}$ with the object $O_i$ at depth $\lambda$, given by \eqref{eq:defineassocP}.

%\begin{align}
%  \assocP &= \Prefl\Ptrans\\
%  \Ereproj(\lambda) &= \left\|\trackpj{t} - \projectionOf{\invProjectionOftm{\trackpj{t-1}, \lambda}}\right\|^2 .
%  \label{eq:reprojerror}
%\end{align}

%The $\projectionOf{.}$ and $\invProjectionOftm{.}$ denote the projection and inverse projection functions that project 3D point to camera image and vice versa. Note that inverse projection $\invProjectionOf{.}$ depend on both the point $\trackp{t}$ and the unknown depth $\lambda$. Also note that the inverse projection is dependent on TP pose at time $t-1$ while the projection depends on pose at time $t$ which can be different.

\input{Source/contBBoxModel}
%\input{Source/squareBBoxModel.tex}


%\subsubsection{Other energies}
%Other energies we use are described in detail in the supplementary material. We briefly describe them here:
%\begin{description}
  %\item[Lane energy ($\Energy{lane}$)] This energy term constrains the orientation of traffic participants to be parallel to the nearest detected lane. The lanes are either detected visually or obtained from GPS and Map information.
  %\item[Transition probability ($\Energy{dyn}$)] We use these energies to constrain the motion of cars to be smooth in linear motion and rotation motion. We also constrain cars to move in the direction of heading.
  % \item[Size prior ($\EnergySize$)] We use size prior of cars by using the mean of cars over the KITTI dataset.
%\end{description}

\vspace{-0.3cm}
\paragraph{Inference on graphical model}
We apply Metropolis Hastings method~\cite{mackay1998introduction} for performing inference on the graphical model. Since we are optimizing over continuous variables, we use Gaussian distribution as the proposal function. We choose this over alternatives such as blocked coordinate descent since they are slower in our experiments.

%in our experiments. Please refer to the supplementary material for our comparisons of inference methods.

%% \begin{figure}
%%   \centering
%%   \newcommand{\imagewidth}{\columnwidth}
%%   \input{Source/scenelayoutoverlayCity0961}
%%   \caption{A sample road scene with the unknowns of each car modeled as random variables. 
%%   The relating energies are shown in Figure~\ref{fig:graphmodel}}
%% \end{figure}
%% \begin{figure}
%%     \input{Source/graphical_model_multiple}
%%     \caption{Graphical model for a single frame with state of car represented
%%     as single node.  The six numbered nodes represent the unknown state variables of each car. The shaded nodes in the graphical model are observed variables. }
%%   \label{fig:graphmodel}
%% \end{figure}
