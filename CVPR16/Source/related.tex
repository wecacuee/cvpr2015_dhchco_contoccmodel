\section{Related Work}
\label{sec:related}

\paragraph{Occlusion handling in detection}
Several works in object detection consider occlusion by training a detector on visible parts of the object \cite{Gao_etal_2011}. Occlusion reasoning based on 2D image silhouettes is used to improve detection performance in \cite{Hsiao_Herbert_2012}. On the other hand, our occlusion reasoning is based on 3D entities. In recent years, object detectors have also considered occlusion reasoning using 3D cues, often learned from a dataset of CAD models \cite{Pepik_etal_2012,Pepik_etal_2013,Xiang_Savarese_2013}. By necessity, such frameworks are often a discrete representation of occlusion behavior, for example, in the form of a collection of occlusion masks derived from object configurations discretized over viewpoint. In contrast to these works, our occlusion modeling is also fully 3D, but allows for a continuous representation. Further, to derive 3D information, we do not use CAD models, rather we derive a probabilistic formulation based on physical insights.


\vspace{-0.3cm}
\paragraph{Occlusion handling in tracking}
Occlusions have also been handled in tracking-by-detection frameworks by considering occluder patterns in the image \cite{Kwak_etal_2012,Wu_Nevatia_2007}. A notable exception is the work of Milan et al.~\cite{Milan_etal_2014} that explicitly models occlusions in the continuous domain to determine a visibility ratio for each object in multi-target tracking. However, the occlusion model in \cite{Milan_etal_2014} is essentially the overlap of image projections of a Gaussian representation of the object. Our occlusion modeling, on the other hand, is fully 3D, based on physical modeling of object-ray intersections and much more general in determining the probability of a point in space as belonging to an object. While our model can also be used to determine a visibility ratio similar to \cite{Milan_etal_2014}, it has far more general applications and can be quantitatively evaluated, as shown by our experiments on point track associations.


%\paragraph{Occlusion handling} 
%Occlusion handling is an integral part of many computer vision applications such as object detection and tracking. For instance, different detectors are trained for specific parts of the object in \cite{Gao_etal_2011} and 3D cues are learned from a CAD database in \cite{Pepik_etal_2013,Xiang_Savarese_2013}. Such prior approaches are application-dependent and either based on 2D cues~\cite{Gao_etal_2011} or offer a discrete representation of occlusion behaviors, for example, in terms of a set of occlusion masks \cite{Pepik_etal_2013,Xiang_Savarese_2013,Zia_etal_2014}. In contrast, our occlusion model is physically-inspired, fully 3D and a continuous representation, which makes it amenable to various applications and enables the use of continuous optimization tools. For object tracking, occlusions are usually handled by a set of occluder patterns~\cite{Kwak_etal_2012,Wu_Nevatia_2007}, which is also a discrete representation. A notable exception is the work of Milan et al.~\cite{Milan_etal_2014}, which explicitly models object occlusions in the continuous domain. However, their work is based on 2D ellipses while we use 3D ellipsoids, which are more principled and meaningful for tackling occlusions in 3D space. Moreover, our formulations have more general applications, especially 3D scene understanding such as 3D localization and reconstruction.


\vspace{-0.3cm}
\paragraph{Motion segmentation and multibody SFM}
An application for our occlusion modeling is to determine point track associations in scenes with multiple objects. For moving objects, this is within the purview of motion segmentation, which has been approached through algebraic factorization methods \cite{Costeria98,Vidal03,Vidal04}, statistical methods~\cite{Kanatani01,Gruber04,Rao08} and clustering methods~\cite{Yan06,Goh07}. Some recent efforts include robust algebraic segmentation with hybrid perspective constraints \cite{Rao_etal_2010} and spectral clustering with point track spatial affinities \cite{Brox_Malik_2010}. Unlike our work, such methods cannot handle static objects, or dynamic objects with little relative motion.
%\cite{Tron_Vidal_2007,Rao_etal_2010}. This also motivates further applications such as object segmentation based on point trajectories \cite{Brox_Malik_2010}. 
Closer to our application, motion segmentation is also used within multibody SFM frameworks \cite{Ozden_etal_2010,Kundu_etal_2011,Namdev2012}. In contrast to these works, our formulation does not distinguish between moving and static objects and also explicitly reasons about occlusions due to 3D object geometries for associating point tracks to individual objects.


%\paragraph{Motion segmentation}
%Motion segmentation has actively been studied in the literature. Previous work can roughly be categorized into algebraic/factorization methods~\cite{Costeria98,Vidal03,Vidal04}, statistical methods~\cite{Kanatani01,Gruber04,Rao08} and clustering methods~\cite{Yan06,Goh07}. Some recent efforts include robust algebraic segmentation with hybrid perspective constraints~\cite{Rao_etal_2010} and spectral clustering with point track spatial affinity~\cite{Brox_Malik_2010}. The above methods usually exploit the fact that point tracks of a particular moving object tend to follow similar motion patterns (e.g., affine, epipolar and homography constraints). Therefore, they cannot segment static objects due to no motion cues. In contrast, our method for point-to-object association is relatively independent of object motions and hence does not suffer from static objects. 


\vspace{-0.3cm}
\paragraph{3D localization}
One of the vital goals of 3D scene understanding is to localize the 3D objects in complex scenes. 
%For instance, using stereo imagery, several visual cues are combined in \cite{Geiger_etal_2014} to simultaneously determine object locations and a rough intersection topology. 
Monocular frameworks like ours have also reasoned about occlusions, for instance, \cite{Wojek_etal_2013} considers partial object detectors. A detailed part-based representation of objects based on annotated CAD models is used for monocular scene understanding in \cite{Zia_etal_2013,Zia_etal_2014}, which also allows reasoning about mutual occlusions between objects. In contrast to these works, our monocular framework uses a physical modeling of occlusion in continuous space and derives unified representations for SFM points and object detection bounding boxes. This makes our model more general, extensible and amenable for continuous optimization.


%One of the models closest to our unified occupancy model was recently presented by Rhodin et al. \cite{Rhodin:2015} applied to the problem of pose estimation.  Although they model transmission probability like we do, they do not model reflectance probability. We explicitly model reflectance probability that differentiates between the center and surface of the 3D object.

%Notable monocular frameworks for 3D localization include \cite{Wojek_etal_2013}, which uses partial object detectors for handling occlusions and \cite{Zia_etal_2013,Zia_etal_2014}, which learn a detailed part-based representation of objects from annotated CAD models for reasoning about mutual occlusions. Generally speaking, the above methods are based on a discrete representation of occlusion behaviors. In contrast, leveraging on our occlusion model, we further develop unified formulations for both point track and bounding box energies, which are continuous and occlusion-aware, leading to an increase in 3D localization accuracy.

%One of the central goals of 3D scene understanding is to localize the 3D positions and orientations of objects in complex scenes. For instance, using stereo imagery, several visual cues are combined in \cite{Geiger_etal_2014} to simultaneously determine object locations and a rough intersection topology. Similar to us, other works have also considered monocular frameworks. Notably, occlusions are explicitly handled in \cite{Wojek_etal_2013} by considering partial object detectors. A detailed part-based representation of objects based on annotated CAD models is used for monocular scene understanding in \cite{Zia_etal_2013,Zia_etal_2014}, which also allows reasoning about mutual occlusions between objects. In contrast to these works, our monocular framework uses a physical modeling of occlusion in continuous space, which makes it more general, extensible and amenable for continuous optimization.
